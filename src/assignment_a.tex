\section*{A – Density modeling\footnote{The associated code for the report is available at the following \href{https://github.com/jsr-p/pmldiku-exam-paper}{Github repository.}}}

\subsection*{A.1 – Implement a convolutional VAE}
The setup of the variational autoencoder model is the assumption of 
a deep latent variable model (DLVM), which is a latent variable model 
$p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})$ whose distributions are parameterized
by a deep neural network \citep{Kingma_2019}. 
We are interested in conducting inference on the posterior distribution 
$p_{\bm{\theta}}(\mathbf{z} \mid  \mathbf{x})$, which
by Bayes' rule can be written as
    \begin{align}
        \label{posterior_dlvm}
        p_{\bm{\theta}}(\mathbf{z} \mid  \mathbf{x})
        = 
    \frac{p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\bm{\theta}}(\mathbf{x})}.
    \end{align}
For DLVMs the marginal likelihood $p_{\bm{\theta}}(\mathbf{x})$ and the posterior $p_{\bm{\theta}}(\mathbf{z} \mid  \mathbf{x})$ are intractable.
However, approximate inference is possible using the variational autoencoder. The variational autoencoder posits a parametric inference model
$q_{\phi}(\mathbf{z} \mid \mathbf{x})$ called the encoder. The vector
$\mathbf{\phi}$ consists of the variational parameters which are optimized such
that the encoder approximates the posterior distribution as shown in equation
\eqref{posterior_dlvm}. The main objective is the evidence lower bound (ELBO):  
\begin{align}
    \label{ELBO}
    \mathcal{L}_{\bm{\theta}, \bm{\phi}}(\mathbf{x})
    &= \mathbb{E}_{q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x})} \left[
        \log p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})
        - \log q_{\phi}(\mathbf{z} \mid \mathbf{x})
    \right] \\
    &= 
    - \mathrm{KL} \left( q_{\phi}(\mathbf{z} \mid \mathbf{x}) \mid  p_{\bm{\theta}}(\mathbf{z}) \right) 
    + 
    \mathbb{E}_{q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x})} \left[
        \log p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z})
    \right].
\end{align}

To optimize the objective in \eqref{ELBO} with respect to $\bm{\phi}$ and $\bm{\theta}$
we use the reparametrization trick \cite{Kingma_2019}.
This allows us to estimate the individual-datapoint ELBO \eqref{ELBO} 
using a simple Monte Carlo estimator:
first we sample a noise term $\bm{\epsilon} \sim p(\bm{\epsilon}) = \mathcal{N}(\bm{\epsilon}; \bm{0}, \bm{I})$ from a multivariate standard normal distribution; then we pass it through a function 
$\mathbf{z} = g(\bm{\phi}, \bm{\theta}, \bm{\epsilon})$ and finally we evaluate 
\begin{align}
    \label{sp-ELBO}
    \tilde{\mathcal{L}}_{\bm{\theta}, \bm{\phi}}(\mathbf{x}) 
    =
    q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x}) -
    \log p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z}).
\end{align}
Using the reparametrization trick the gradient $
    \nabla_{\bm{\theta}, \bm{\phi}}\tilde{\mathcal{L}}_{\bm{\theta}, \bm{\phi}}(\mathbf{x}) 
$
is readily optimized using the autodifferentiation framework of PyTorch \cite{pytorch}.

\subsubsection*{Factorized Gaussian Encoder}
In all of our models we assume that the encoder 
is a Gaussian encoder with diagonal covariance matrix 
\begin{align}
    q_{\phi}(\mathbf{z} \mid \mathbf{x}) 
    &= \mathcal{N}(\mathbf{z}; \bm{\mu}, \bm{\sigma}^2 \mathbf{I}) \\ 
    (\bm{\mu}, \log \bm{\sigma}^2) 
    &= \mathrm{EncoderNeuralNet}_{\bm{\phi}}(\mathbf{x}).
\end{align}

The resulting estimator for a single datapoint equals 
\begin{align}\label{eq7}
    \tilde{\mathcal{L}}_{\bm{\theta}, \bm{\phi}}(\mathbf{x}) 
    &=
    \frac{1}{2} \sum_{j=1}^{J} \left( 
    1 + \log \sigma^2_j - \mu_{j} - \sigma^2_{j}
    \right) 
    + \log p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z}) \\ 
    \text{where  }  \mathbf{z} &= \bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}, \ \
    \bm{\epsilon} \sim  \mathcal{N}(\bm{0}, \bm{I}).
\end{align}

\subsubsection*{Decoder}
The term  $\log p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z})$ takes three different forms depending on our three separate assumptions – that $p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z})$ is (1) Gaussian  distributed, (2) Bernoulli distributed or (3) Continuous Bernoulli  distributed. Assumption (1) is motivated by the fact that the standardized MNIST data is continuous in the $[0,1]$-interval. However in practice, the distribution of the pixels in MNIST are largely centered on 0 and 1 favoring the Bernoulli assumption. These two considerations jointly motivate the third assumption: the assumption of the pixels following a continuous Bernoulli (CB) distribution \cite{CB}. This is a novel distribution specifically created because of the considerations of the pixels neither being normal or Bernoulli distributed. Instead, the CB distribution models a continuous variable in $[0, 1]$-interval; exactly as the digits in the normalized MNIST data.  
Considering the loss function used in estimating the VAE, under assumption 1. we use the mean-squared error (MSE) for the reconstruction error in \eqref{eq7}, whereas 2. implies binary cross-entropy loss (BCE), and 3. implies BCE plus the normalizing constant of the continuous Bernoulli distribution (see \cite{CB}). 
For all of the three assumptions we model the decoder as a parametrized neural net.



\subsection*{A.2 – Alternative models}
\subsubsection*{Fully Bayesian VAE}
As written in \citep{BVAE}, a full Bayesian analysis of the VAE is possible. This analysis allows us to perform variational inference on
the latent variables $\mathbf{z}$ and the parameters of $\bm{\theta}$.

We assume a hyperprior on the decoder's parameters; we assume the hyperprior has a multivariate standard normal density $p_{\bm{\alpha}}(\bm{\theta})$. 
The approximate posterior of $\bm{\theta}$ is denoted by $q_{\bm{\phi}}(\bm{\theta})$. 
In order to perform variational inference and make 
$q_{\bm{\phi}}(\bm{\theta})$ a good approximation to 
 $p_{\bm{\alpha}}(\bm{\theta} \mid \mathbf{X})$ we have to backpropagate through the parameters of the approximate posterior $q_{\bm{\phi}}(\bm{\theta})$. 
 To make this operational we apply the reparametrization trick a second time. 
The loss for a single datapoint is now given by 
\begin{align}
    \label{elbobayes}
	\tilde{\mathcal{L}}_{\bm{\theta}, \bm{\phi}}(\mathbf{x})
    &= 
	\log p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z})
    + 
	\log p_{\bm{\theta}}(\mathbf{x})
    - 
	q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x})
    + 
	\log p_{\bm{\alpha}}(\mathbf{x})
    - 
	\log q_{\bm{\phi}}(\mathbf{z}), \\ 
    \nonumber
	\text{where  }  
    \mathbf{z} & = \bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}, \ \
    \bm{\theta} = \bm{\mu}_{\bm{\theta}} 
    + \bm{\sigma}_{\bm{\theta}} \odot \bm{\zeta} , \ \
	\bm{\epsilon}, \ \bm{\zeta}  \sim  \mathcal{N}(\bm{0}, \bm{I}).
\end{align}
The loss function is identical to the one in \eqref{eq7} 
with the the term
$\log q_{\bm{\phi}}(\mathbf{z})$
added. The same assumptions as before apply to the term $\log p_{\bm{\theta}}(\mathbf{x} \mid \mathbf{z})$.

\subsubsection*{Denoising Diffusion Probabilistic Models}
We implement a diffusion model as a SOTA generative image model \cite{diffusion2020}. We briefly give a summary of the model. Fundamentally, the model's encoding part consists of additively adding Gaussian noise to images. This goes on for $T$ steps until the original image is indistinguishable from Gaussian noise. More precisely we can consider the noise process a Markov chain of the form as described in \cite[p. 860, eq. 25.1]{pml2Book}:

\begin{equation}
    q(x_t \mid x_{t-1}) = \mathcal{N}(x_t \mid \sqrt{1 - \beta_t} x_{t-1}, \beta_t\mathbf{I})
\end{equation}

$\beta_t \in (0, 1)$ is chosen to follow a schedule, which defines the amount of noise. Transforming these $\beta_t$'s yields $\alpha_t = 1 - \beta_t$ and from here we define $ \Pi_{s=1}^t \alpha_s$. Using this we can construct our loss function:

\begin{equation}
    \lambda_t\Vert \epsilon - \epsilon_\theta (\sqrt{\bar{\alpha}} x_0 + \sqrt{1-\bar{\alpha}}\epsilon, t) \Vert^2
\end{equation}

As described in \cite[p.863, eq. 25.23 and 25.24]{pml2Book} the model performs best when $\lambda_t$ is set to 1. In essence, we can consider the prediction problem as removing noise from a noisy image i.e. going from $x_t$ to $x_{t-1}$. From here we can consider the denoising problem as starting by sampling an "image" of multivariate Gaussian noise, and iteratively using the trained neural network. More formally, we can consider the de-noising problem as $p_{\theta}(x_{t-1}\mid x_t) = \mathcal{N}\left(x_{t-1} ; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)\right)$, where $\mu_\theta$ and $\Sigma_\theta$ denotes the neural parametrization of a neural network. We use a convolutional neural network to de-noise the images, where we embed the time-step dimension using sinusoidal transformation because neural networks struggle to embed time.
We sample  $t\sim \mathcal{U}(0, T), \epsilon \sim \mathcal{N}(0, 1)$ and we choose the $T=1000, \beta_{min} = 10^{-4}, \beta_{max} = 0.2$ as done in the original paper \cite{diffusion2020}, and we also generate the $\beta$'s with equal distance. We use both a standard convolutional net, and a U-Net architecture for the decoding neural network. We use the empirical mean and standard deviation for scaling inputs\footnote{Scaling the inputs turns out to be incredibly important. Originally we scaled by the pixels so min = -1 and max = 1, but we found the performance increased using the empirical mean and standard deviation of the data. The original paper uses a more intricate scaling procedure \cite{diffusion2020}, which probably could improve performance more had we had more time.}. 

\subsection*{Assessing Generative Models}

We consider three ways of assessing our general models: Visual inspection, marginal likelihood in the case of VAE-type models, and as a last measure we use the Fréchet inception distance (FID). 

\textbf{Visual Inspection:} In general, we find that the VAEs outperform the diffusion models. In particular, the simple convolutional implementation of a diffusion model seems to be totally unable to generate images. The U-Net implementation does fairly well at generating realistic digits, and one could argue that the quality of the digits are seemingly higher than the quality of digits that some of the VAEs can produce. For the baseline VAE, we find that the continuous Bernoulli assumption increases the quality of the generated images significantly. This also appears to be the best performing model for generating digits. The lower quality of the convolutional VAE may stem from the fact that we have considered an oversimplified convolutional network. 
Lastly, the Bayesian VAE is hit or miss with regards to the quality of generated images. This suggests that this model manages to model the latent space well for some regions but not for others, implying that the KL-part of the loss function not regularizing the embedding enough to a standard normal distribution. A way to address this could be to choose a prior that has more weight on the KL term, akin to what is done \cite{Higgins2016betaVAELB}. Further, the complex nature of a fully Baeysian VAE, and the many variational parameters to estimate, may also be the reason that it is difficult to train and get reasonable generative results.

\textbf{Marginal Likelihood:} The marginal (log) likelihood as a tool for model selection can be motivated by considering a posterior distribution over models, $p(m|X)$, in which we assume a uniform prior over models. Computing the MAP estimate of $p(m|X)$ is then equivalent to selecting the model with the highest marginal likelihood, $p(X|m)$. To estimate the marginal log-likelihood of the VAE models we use importance sampling as suggested in \cite{importancesampling}. The marginal log-likelihood can be rewritten as

\begin{align}
	\nonumber
	\log p_{\bm{\theta}}(\mathbf{x}) = \log \int p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})
	\frac{q_{\phi}(\mathbf{z} \mid \mathbf{x})}{q_{\phi}(\mathbf{z} \mid \mathbf{x})}
	d \mathbf{z}                                                                                      
	                                 & =
	\nonumber
	\log \mathbb{E}_{q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x})} \left[
		\frac{
			p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})
		}{q_{\phi}(\mathbf{z} \mid \mathbf{x})}
	\right]                                                                                             \\
	\label{IS-VAE}
	\Leftrightarrow
	\log p_{\bm{\theta}}(\mathbf{x}) & \approx
	\log \frac{1}{L} \sum_{l=1}^{L}
	\frac{
		p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z}^{(l)}) p_{\bm{\theta}}(\mathbf{z})
	}{
		q_{\phi}(\mathbf{z}^{(l)} \mid \mathbf{x})
	}
\end{align}

\iffalse

\begin{align}
	\nonumber
	\log p_{\bm{\theta}}(\mathbf{x}) & = \log \int p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})
	\frac{q_{\phi}(\mathbf{z} \mid \mathbf{x})}{q_{\phi}(\mathbf{z} \mid \mathbf{x})}
	d \mathbf{z}                                                                                        \\
	                                 & =
	\nonumber
	\log \mathbb{E}_{q_{\bm{\phi}}(\mathbf{z} \mid  \mathbf{x})} \left[
		\frac{
			p_{\bm{\theta}}(\mathbf{x}, \mathbf{z})
		}{q_{\phi}(\mathbf{z} \mid \mathbf{x})}
	\right]                                                                                             \\
	\label{IS-VAE}
	\Leftrightarrow
	\log p_{\bm{\theta}}(\mathbf{x}) & \approx
	\log \frac{1}{L} \sum_{l=1}^{L}
	\frac{
		p_{\bm{\theta}}(\mathbf{x} \mid  \mathbf{z}^{(l)}) p_{\bm{\theta}}(\mathbf{z})
	}{
		q_{\phi}(\mathbf{z}^{(l)} \mid \mathbf{x})
	}
\end{align}

\fi

where $\mathbf{z}^{(l)} \sim q_{\phi}(\mathbf{z} \mid \mathbf{x}) $. 
Therefore, to estimate the marginal log likelihood for a single datapoint $\mathbf{x}$
we draw $L$ observations from the encoder and compute the expression in \eqref{IS-VAE}.
To get the estimate on the total test data we compute the expression in \eqref{IS-VAE} 
for each data point and take the average. 
To avoid numerical instability we use the log-sum-exp trick. Looking to table \ref{tab:loglik} we find that the bayesian model performers worst within each group of error distribution specification\footnote{\textit{MSE} denotes mean squared error, \textit{cb} denotes continuous Bernoulli, \textit{BCE} denotes binary cross-entropy}.

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{llllllllll}
    \toprule
    model       &  vanilla mse &  conv mse &  bayes mse &  vanilla bce &  conv bce &  bayes bce &  vanilla cb &  conv cb &  bayes cb \\
    \midrule
    $\log p(x)$ &       -11.63 &    -11.79 &     -31.97 &       -41.53 &    -42.51 &     -61.81 &     2033.23 &  2063.43 &   1988.12 \\
    \bottomrule
    \end{tabular}}
    \caption{Marginal log likelihood}
    \label{tab:loglik}
\end{table}
\textbf{Fréchet Inception Distance:} The FID is found by measuring the distance in the embedding space of a classifier between real data and generated data. More concretely we train a convolutional neural network on the MNIST data\footnote{It predicts the correct label with a $97\%$ probability.}, and use the last embedding layer before the prediction to calculate the distance between the real (denoted $R$) and generated (denoted $G$) assuming both follow normal distributions. 

\begin{equation}
    FID = \Vert \mu_R - \mu_G \Vert_2^2 + \mathrm{tr}(\Sigma_R + \Sigma_G - 2 (\Sigma_R \Sigma_G)^{1/2})
\end{equation}

Note that lower distance implies a higher quality of generated images. Following \cite[p. 775]{pml2Book} the score has shown to be sensitive to sample size so for all models we sample 10000 images to calculate the distance. The embedding layer used for estimating the FID is 10-dimensional. The resulting scores are:
\begin{table}[ht]
    \centering
    \input{code/tables/fso_comparison.tex}
    \caption{Fréchet Inception Distance}
    \label{tab:FID}
\end{table}

Table \ref{tab:FID} shows the FIDs\footnote{We only show a subset of VAE models. The models all use continuous Bernoulli for the reconstruction loss.}. We find that the best VAE's outperform diffusion models as measured by FID score. However, these measures are biased due to the distribution of $X$ in the images generated by the diffusion models. The samples in these models have been standardized, but can still take values beyond 0 and 1. We tried to use CLIP in the training stage, but could not get the models to perform well. Again, with more time, standardizing the data as input, and correctly specifying some function that generates the output, would make for more fair comparisons. In other words, for this method to be. In other words, for this method to work well, especially for diffusion models, more care should have been put into how to specify the model, so the output would follow the distribution observed empirically.